{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie recommendation engine.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phanindraarun911/PROJECTS-99/blob/main/Movie_recommendation_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lFgIisQ33AV-"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.metrics.pairwise import linear_kernel,cosine_similarity"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DWaEWsP3jLvn",
        "outputId": "c9d577f4-e174-464f-8418-5ce41427f8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "m7kTy86Mhrnr"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "import string\n",
        "from requests import get\n",
        "\n",
        "#Library for Collaborative filtering\n",
        "!pip install surprise\n",
        "from surprise import Reader,Dataset,SVD,evaluate\n",
        "import warnings;warnings.simplefilter('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8eyWDGrhrl_"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GUjN2CzGhrh5"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata=pd.read_csv('movies_metadata.csv',error_bad_lines=False)\n",
        "movies_metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_RbDpbyAhrf4"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irH8BOYmhrdt"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o28-6jLIn0WX"
      },
      "cell_type": "markdown",
      "source": [
        "# Lets explore the movies metadata"
      ]
    },
    {
      "metadata": {
        "id": "Dlg31wpynnTk"
      },
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots()\n",
        "fig.set_size_inches(12,9)\n",
        "sns.heatmap(movies_metadata.isnull(),yticklabels=False,cmap='viridis',ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wE2j2YHnnR0"
      },
      "cell_type": "code",
      "source": [
        "#From the graph we can easily visualize how much daa is missing for our dataset ,We have lots of data is missing in tagline ,belongs to collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bxlH7gLGnnNt"
      },
      "cell_type": "code",
      "source": [
        "#Reading the movie from our small movies data set\n",
        "movies_small=pd.read_csv('movies.csv')\n",
        "movies_small.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CU6jcXEDnnJJ"
      },
      "cell_type": "code",
      "source": [
        "movies_small.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHnh1LewnnGf"
      },
      "cell_type": "code",
      "source": [
        "#Cheking null values in dataset\n",
        "movies_small.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rvYXRfISnnDv"
      },
      "cell_type": "code",
      "source": [
        "# Creating a plot for Genre Distribution\n",
        "df1 = movies_small['genres'].apply(lambda genrelist : str(genrelist).split(\"|\"))\n",
        "df1 = pd.Series(df1).apply(frozenset).to_frame(name='givengenres')\n",
        "for givengenres in frozenset.union(*df1.givengenres):\n",
        "    df1[givengenres] = df1.apply(lambda _: int(givengenres in _.givengenres), axis=1)\n",
        "df1.drop('givengenres',axis=1,inplace=True)\n",
        "df1['movieId']=movies_small['movieId']\n",
        "df1 = pd.merge(movies_small,df1,on='movieId')\n",
        "df1.head()\n",
        "genre_columns= ['Film-Noir',\n",
        "       'Romance', 'Western', 'Documentary', 'Thriller', 'Action', 'Musical',\n",
        "       'War', 'Drama', 'IMAX', 'Crime', 'Children', 'Adventure', 'Horror',\n",
        "       'Fantasy', 'Animation', 'Comedy', 'Mystery', '(no genres listed)',\n",
        "       'Sci-Fi']\n",
        "df1[genre_columns].sum().sort_values(ascending=False).plot(kind='bar',figsize=(12,9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hC9AVS8Tq-PP"
      },
      "cell_type": "markdown",
      "source": [
        "We can see from the above graph that majority of the movies are of Drama , comedy and thriller"
      ]
    },
    {
      "metadata": {
        "id": "On9z-hahrQAX"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Plotting the graph to see the distribution of votes across"
      ]
    },
    {
      "metadata": {
        "id": "eK3Gz14Fq8qv"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#plt.figure(figsize=\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x7vr-Sw2sRPp"
      },
      "cell_type": "markdown",
      "source": [
        "most of the vote counts is between 0-5000"
      ]
    },
    {
      "metadata": {
        "id": "qbXzvkGhq8o-"
      },
      "cell_type": "code",
      "source": [
        "# Look into the distribution of vote average for out movies dataset\n",
        "plt.figure(figsize=(12,6))\n",
        "movies_metadata['vote_average'].plot(kind='hist')\n",
        "plt.xlabel('Vote average')\n",
        "plt.ylabel('Number of movies')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ME4Y08KtK_H"
      },
      "cell_type": "markdown",
      "source": [
        "This is IMDB rating here we can visualize that most of the vote is between 5 to 7"
      ]
    },
    {
      "metadata": {
        "id": "NwFYjjIXq8m3"
      },
      "cell_type": "code",
      "source": [
        "#Let's create a join plot to see the voete counts and vote average distribution\n",
        "sns.jointplot(x='vote_average',y='vote_count',data=movies_metadata,alpha=0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgP3nyCKuFHE"
      },
      "cell_type": "markdown",
      "source": [
        "From here we can refer one thing that movie whcih have higher rating has higher vote count means more people watch and rate popular movies"
      ]
    },
    {
      "metadata": {
        "id": "sZZmKYmAq8jD"
      },
      "cell_type": "code",
      "source": [
        "links=pd.read_csv('links.csv')\n",
        "links.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YzgIT49Gq8gu"
      },
      "cell_type": "code",
      "source": [
        "links.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ustEPPwVq8eG"
      },
      "cell_type": "code",
      "source": [
        "#Checking Null values\n",
        "links.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WsnwiLWIq8cZ"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.id.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdu0VQQMq8ai"
      },
      "cell_type": "code",
      "source": [
        "links.tmdbId.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "95U3zcFmwFe3"
      },
      "cell_type": "code",
      "source": [
        "#Since we are using movies small dataset, we will only keep values in movies_metadata for movies in movies_small\n",
        "movies_metadata = movies_metadata[movies_metadata.id.isin(links['tmdbId'].astype(str).apply(lambda x:x[:-2]).tolist())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YC6Vvf54q8Yb"
      },
      "cell_type": "code",
      "source": [
        "#Lets look at the null values after we have created this smaller version of movies_metadata\n",
        "fig, ax = plt.subplots()\n",
        "# the size of A4 paper\n",
        "fig.set_size_inches(11.7, 8.27)\n",
        "sns.heatmap(movies_metadata.isnull(),yticklabels=False,cbar=False,cmap='viridis',ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KK_u1v_Yq8UV"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['genres'] = movies_metadata['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TU9vLDfrwnYX"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to add movie id to our movies metadata, we can do it through links.csv"
      ]
    },
    {
      "metadata": {
        "id": "oo4Bj6J0wjXC"
      },
      "cell_type": "code",
      "source": [
        "def convert_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qtGzCZVJwjUa"
      },
      "cell_type": "code",
      "source": [
        "links['tmdbId'] = links['tmdbId'].apply(convert_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wsw5nO_uwjRj"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['id'] = movies_metadata['id'].apply(convert_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_yG-zORmwjO0"
      },
      "cell_type": "code",
      "source": [
        "def return_movieId(tmdbId):\n",
        "    return links[links['tmdbId']==tmdbId]['movieId'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZjBUV3OmwjMX"
      },
      "cell_type": "code",
      "source": [
        "#Get movie Id to the movies_metadata\n",
        "movies_metadata['movieId'] = movies_metadata['id'].apply(return_movieId)\n",
        "movies_metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iYcN3ALiwjJg"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DY4ff5TdyVX7"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Since some movies may have low vote average but more number of votes, while other movies may have high vote average and less vote counts, We need a common medium to sort the movies to create top movies chart. For this, let's use IMDB's weighted rating formula to construct top movies chart. Mathematically, it is represented as follows:\n",
        "\n",
        "Weighted Rating (WR) = (v/(v+m)) R+(m/(v+m)) C\n",
        "\n",
        "Where,\n",
        "R = average for the movie (mean) = (Rating)\n",
        "v = number of votes for the movie = (votes)\n",
        "m = minimum votes required to be listed in the\n",
        "C = the mean vote across the whole report\n",
        "\n",
        "The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. We will use 95th percentile as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 95% of the movies in the list.\n",
        "\n",
        "I will build our overall Top 100 Chart and will define a function to build charts for a particular genre. Let's begin!\n",
        "\n",
        "<<<<< enter link for IMDB weighted rating here\n"
      ]
    },
    {
      "metadata": {
        "id": "WodlVkS8wjHl"
      },
      "cell_type": "code",
      "source": [
        "no_of_votes = movies_metadata[movies_metadata['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "vote_mean = movies_metadata[movies_metadata['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "C = vote_mean.mean()\n",
        "C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZeVxNZLwjFY"
      },
      "cell_type": "code",
      "source": [
        "m = no_of_votes.quantile(0.95)\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ywzM3FPKwjDN"
      },
      "cell_type": "code",
      "source": [
        "# Adding year column based on movies release date\n",
        "movies_metadata['year'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOhPAClTwjAF"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['year'] = pd.to_datetime(movies_metadata['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n",
        "top_movies = movies_metadata[(movies_metadata['vote_count'] >= m) & (movies_metadata['vote_count'].notnull()) & (movies_metadata['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\n",
        "top_movies['vote_count'] = top_movies['vote_count'].astype('int')\n",
        "top_movies['vote_average'] = top_movies['vote_average'].astype('int')\n",
        "top_movies.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLhgFql9KEhj"
      },
      "cell_type": "markdown",
      "source": [
        "Therefore, to qualify to be considered for the chart, a movie has to have at least 2079 votes on TMDB. We also see that the average rating for a movie on TMDB is 5.916 on a scale of 10. 455 Movies qualify to be on our chart."
      ]
    },
    {
      "metadata": {
        "id": "atSbCdqTKOCA"
      },
      "cell_type": "code",
      "source": [
        "def weighted_rating(x):\n",
        "  V=x['vote_count']\n",
        "  R=x['vote_average']\n",
        "  return (V/(V+m)*R)+(m/(m+V)*C)\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QK96p3bKN3Y"
      },
      "cell_type": "code",
      "source": [
        "top_movies['wr']=top_movies.apply(weighted_rating,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lI4PGCfpwi7e"
      },
      "cell_type": "code",
      "source": [
        "top_movies = top_movies.sort_values('wr', ascending=False).head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-d7afNOohrZl"
      },
      "cell_type": "code",
      "source": [
        "top_movies.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wj9gry3gLYNV"
      },
      "cell_type": "code",
      "source": [
        "top_movies.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H9g7gcUSLYKe"
      },
      "cell_type": "code",
      "source": [
        "y = movies_metadata.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "y.name = 'genre'\n",
        "gen_data = movies_metadata.drop('genres', axis=1).join(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-QC7cuipMl9h"
      },
      "cell_type": "markdown",
      "source": [
        "Let us now construct our function that builds charts for particular genres. For this, we will use relax our default conditions to the 85th percentile instead of 95."
      ]
    },
    {
      "metadata": {
        "id": "VS62cRWTLYIA"
      },
      "cell_type": "code",
      "source": [
        "def top_movies_genre(genre, percentile=0.85):\n",
        "    df = gen_data[gen_data['genre'] == genre]\n",
        "    no_of_votes = df[df['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "    vote_mean = df[df['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "    C = vote_mean.mean()\n",
        "    m = no_of_votes.quantile(percentile)\n",
        "\n",
        "    top_movies = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]\n",
        "    top_movies['vote_count'] = top_movies['vote_count'].astype('int')\n",
        "    top_movies['vote_average'] = top_movies['vote_average'].astype('int')\n",
        "\n",
        "    top_movies['wr'] = top_movies.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C), axis=1)\n",
        "    top_movies = top_movies.sort_values('wr', ascending=False).head(100)\n",
        "\n",
        "    return top_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o-uDSp1nLYE6"
      },
      "cell_type": "code",
      "source": [
        "top_movies_genre('Animation').head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K1M5i5XqNCQV"
      },
      "cell_type": "markdown",
      "source": [
        "## Content Based Recommender\n",
        "The simple recommender that we just built provides just the top results for the genre, and it shows the the same results for every user looking for that genre.\n",
        "\n",
        "It also dosen't account for fan following towards particular director or Actors, which accounts for people also watching the movies that are less popular but from famous actors and directors.\n",
        "\n",
        "For personalized recommendations, We will create a recommendation system that computes similarity between movies based on certain features and recommend movies that are similar to user's taste. As we are using movie's metadata (or content) for creating this system, it is also referred as Content Based Filtering.\n",
        "\n",
        "We will build four Content Based Recommenders based on:\n",
        "\n",
        "- Movie overview's that particular user has liked and use latent semantic similarity for comparing similar movies\n",
        "- Adding Taglines and Movie Overviews and compare using pairwise cosine similarity\n",
        "- Movie Cast, Crew, Keywords and Genre"
      ]
    },
    {
      "metadata": {
        "id": "BHuI4maMNH1c"
      },
      "cell_type": "markdown",
      "source": [
        "#### Movie Description Based Recommender\n",
        "Let us first try to build a recommender using movie descriptions and taglines. We do not have a quantitative metric to judge our machine's performance so this will have to be done qualitatively."
      ]
    },
    {
      "metadata": {
        "id": "99D-uGIeLYCW"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "16hqukJwNXfz"
      },
      "cell_type": "markdown",
      "source": [
        "For our first attempt in building Description based recommendation system,\n",
        "- We will first take list of movies which a user has watched\n",
        "- Process the description of the movie using NLP techniques like removing stopwords and punctuations, applying Tokenization, lemmatization and stemming, and return a clean list of words\n",
        "- Using similar techniques, we will process the description/overview of movie in our movies_metadata for the top 80 percentile of movies\n",
        "- In the next step, we will calculate the similarity between the combined overview of the movies user has watched and the overview of the movies user hasn't watched\n",
        "- To get this similarity, we will use UMBC's API service to provide latent semantic similarity between 2 scentences. The link to which can be found [here](http://swoogle.umbc.edu/SimService/api.html)"
      ]
    },
    {
      "metadata": {
        "id": "iICmmfmUNfyL"
      },
      "cell_type": "markdown",
      "source": [
        "Lets get the top movies that a user has rated more than average"
      ]
    },
    {
      "metadata": {
        "id": "_4llvH8tLX_b"
      },
      "cell_type": "code",
      "source": [
        "ratings=pd.read_csv('ratings.csv')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ebCg7oeANY0P"
      },
      "cell_type": "code",
      "source": [
        "#Get movieId for above average ratings for userId 1\n",
        "ratings[(ratings['userId']==1) & (ratings['rating']>2.5)]['movieId'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eJvqOlxRLX8W"
      },
      "cell_type": "code",
      "source": [
        "def text_process(mess):\n",
        "    \"\"\"\n",
        "    1. remove punc\n",
        "    2. remove stop words\n",
        "    3. apply lemmatization\n",
        "    4. apply stemmization\n",
        "    5. return list clean overview\n",
        "\n",
        "    \"\"\"\n",
        "    #Remove Stopwords and punctuations\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "    stopwords = nltk.corpus.stopwords.words('english')\n",
        "    nopunc = ''.join(nopunc)\n",
        "\n",
        "    #Apply tokenization\n",
        "    tokenized_list = []\n",
        "    tokenized_list =  [word for word in nopunc.split() if word.lower() not in stopwords]\n",
        "\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "    #Applying Lemmatization\n",
        "\n",
        "    lemmatized_words = []\n",
        "    for word in tokenized_list:\n",
        "        lemmatized_words.append(wordnet_lemmatizer.lemmatize(word))\n",
        "\n",
        "   #Applying Stemmization\n",
        "\n",
        "    cleaned_list  = []\n",
        "    for word in lemmatized_words:\n",
        "        cleaned_list.append(snowball_stemmer.stem(word))\n",
        "    return ' '.join(cleaned_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhkRmsdaLX5o"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "movies_metadata['overview'] = movies_metadata['overview'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lG-dCv8bOD3I"
      },
      "cell_type": "code",
      "source": [
        "# Pre-processing the overviews for all the movies\n",
        "movies_metadata['pro_overview'] = movies_metadata['overview'].apply(text_process)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxP1s3nNODzu"
      },
      "cell_type": "code",
      "source": [
        "percentile = 0.90\n",
        "no_of_votes = movies_metadata[movies_metadata['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "vote_mean = movies_metadata[movies_metadata['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "C = vote_mean.mean()\n",
        "m = no_of_votes.quantile(percentile)\n",
        "\n",
        "top_movies = movies_metadata[(movies_metadata['vote_count'] >= m) & (movies_metadata['vote_count'].notnull()) & (movies_metadata['vote_average'].notnull())][['movieId','title', 'year', 'vote_count', 'vote_average', 'popularity','pro_overview']]\n",
        "top_movies['vote_count'] = top_movies['vote_count'].astype('int')\n",
        "top_movies['vote_average'] = top_movies['vote_average'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FRCQf9B_ODwn"
      },
      "cell_type": "code",
      "source": [
        "top_movies.sort_values(by='vote_count',ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ZtiV6LWODtt"
      },
      "cell_type": "code",
      "source": [
        "# Using UMBC's API service to get latent sematic similarity score\n",
        "sss_url = \"http://swoogle.umbc.edu/SimService/GetSimilarity\"\n",
        "\n",
        "def sss(s1, s2, type='relation', corpus='webbase'):\n",
        "    try:\n",
        "        response = get(sss_url, params={'operation':'api','phrase1':s1,'phrase2':s2,'type':type,'corpus':corpus})\n",
        "        return float(response.text.strip())\n",
        "    except:\n",
        "        #print ('Error in getting similarity for %s: %s' % ((s1,s2), response))\n",
        "        return 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XoLjOUbUODqg"
      },
      "cell_type": "code",
      "source": [
        "user_1_movies=[]\n",
        "for movieId in ratings[(ratings['userId']==1) & (ratings['rating']>2.5)]['movieId'].tolist():\n",
        "    user_1_movies.append(movies_metadata[movies_metadata['movieId']==movieId]['pro_overview'].iloc[0])\n",
        "user_1_movies = ' '.join(user_1_movies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M66DxzufODjW"
      },
      "cell_type": "code",
      "source": [
        "user_1_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hpSw7ThsXOOP"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "top_movies['similarity'] = top_movies['pro_overview'].apply(lambda x:sss(user_1_movies,x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yL-Yux1iXOFX"
      },
      "cell_type": "code",
      "source": [
        "top_movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OM10-57YXN7U"
      },
      "cell_type": "code",
      "source": [
        "top_movies[top_movies.movieId.isin(ratings[ratings['userId']!=1]['movieId'].tolist())][['title','similarity','vote_count','vote_average']].sort_values(by='similarity',ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dlieA56yXNz3"
      },
      "cell_type": "code",
      "source": [
        "#Let's create a recommender based on the above method\n",
        "def user_taste_recommender(userId,percentile = 0.90):\n",
        "    no_of_votes = movies_metadata[movies_metadata['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "    vote_mean = movies_metadata[movies_metadata['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "    C = vote_mean.mean()\n",
        "    m = no_of_votes.quantile(percentile)\n",
        "\n",
        "    top_movies = movies_metadata[(movies_metadata['vote_count'] >= m) & (movies_metadata['vote_count'].notnull()) & (movies_metadata['vote_average'].notnull())][['movieId','title', 'year', 'vote_count', 'vote_average', 'popularity','pro_overview']]\n",
        "    top_movies['vote_count'] = top_movies['vote_count'].astype('int')\n",
        "    top_movies['vote_average'] = top_movies['vote_average'].astype('int')\n",
        "\n",
        "    user_movies=[]\n",
        "    for movieId in ratings[(ratings['userId']==userId) & (ratings['rating']>2.5)]['movieId'].tolist():\n",
        "        user_movies.append(movies_metadata[movies_metadata['movieId']==movieId]['pro_overview'].iloc[0])\n",
        "    user_movies = ' '.join(user_movies)\n",
        "\n",
        "    top_movies['similarity'] = top_movies['pro_overview'].apply(lambda x:sss(user_movies,x))\n",
        "    top_movies = top_movies[top_movies.movieId.isin(ratings[ratings['userId']!=userId]['movieId'].tolist())][['title','similarity','vote_count','vote_average']].sort_values(by='similarity',ascending=False).head(10)\n",
        "\n",
        "    return top_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_xdPmOnfTkZ"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "user_taste_recommender(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Z5ejiZLZRSP"
      },
      "cell_type": "markdown",
      "source": [
        "Just getting recommendation based on movie's synopsis dosent provide eye catching results, and is not reliable enough as Latent Semantic Similarity here takes into account movies from all the genre and most importantly, takes lot of time to calculate through the UMBC's API for soo many movies, So we will try Consine similarity from sklearn's linear kernel which is much faster to calculate"
      ]
    },
    {
      "metadata": {
        "id": "cESoq-NCZX-X"
      },
      "cell_type": "markdown",
      "source": [
        "Also, lets add tagline to the description and check if we get better recommendations"
      ]
    },
    {
      "metadata": {
        "id": "2_PTfaRvZL12"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['tagline'] = movies_metadata['tagline'].fillna('')\n",
        "movies_metadata['description'] = movies_metadata['pro_overview'] + movies_metadata['tagline']\n",
        "movies_metadata['description'] = movies_metadata['description'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCWXLxPMZLzc"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['description'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mkS9vk3gZLwR"
      },
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(movies_metadata['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RfxtitqmZ8Ey"
      },
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DroBaz2kZyHQ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cosine Similarity\n",
        "I will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Mathematically, it is defined as follows:\n",
        "\n",
        "cosine(x,y)=x.y⊺||x||.||y||\n",
        "Since we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score. Therefore, we will use sklearn's linear_kernel instead of cosine_similarities since it is much faster."
      ]
    },
    {
      "metadata": {
        "id": "jLBWWFiWZLtS"
      },
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qku0YOiBZLp_"
      },
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ouea8R3jXNsm"
      },
      "cell_type": "code",
      "source": [
        "We now have a pairwise cosine similarity matrix for all the movies in our dataset. The next step is to write a function that returns the 30 most similar movies based on the cosine similarity score."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OBxAhLjXNjx"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata = movies_metadata.reset_index()\n",
        "titles = movies_metadata['title']\n",
        "indexes = pd.Series(movies_metadata.index, index=movies_metadata['title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5T-m5JfuaWgd"
      },
      "cell_type": "code",
      "source": [
        "#To get pairwise similarity score for movie with index 0\n",
        "similarity =  list(enumerate(cosine_sim[0]))\n",
        "print(similarity[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3x3dz6naazw"
      },
      "cell_type": "code",
      "source": [
        "def desc_based_recommendation(title):\n",
        "    idx = indexes[title]\n",
        "    sim = list(enumerate(cosine_sim[idx]))\n",
        "    #Sorting the list by descending order of similarity\n",
        "    sim = sorted(sim, key=lambda x: x[1], reverse=True)\n",
        "    #Taking top 30 similar movies\n",
        "    sim = sim[1:31]\n",
        "    rec_movies_indexes = [i[0] for i in sim]\n",
        "    return titles.iloc[rec_movies_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqIkJwOWamA-"
      },
      "cell_type": "markdown",
      "source": [
        "Let's check recommendatio for start war"
      ]
    },
    {
      "metadata": {
        "id": "TIJ0zAseaaxQ"
      },
      "cell_type": "code",
      "source": [
        "desc_based_recommendation('Star Wars').head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xPkqYQZ7axKx"
      },
      "cell_type": "markdown",
      "source": [
        "We get Return of Jedi and Star Wars: The Force Awakens as a recommendation for star wars which is goodl!"
      ]
    },
    {
      "metadata": {
        "id": "1QdHbZ6Yaauo"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLXdnqoXa39k"
      },
      "cell_type": "markdown",
      "source": [
        "#### Metadata Based Recommender\n",
        "\n",
        "Lets add more details like cast, crew , directors, keywords etc to get better similarity score for movies with similar content.\n",
        "To do the same we need to prepare this data as our first step."
      ]
    },
    {
      "metadata": {
        "id": "WFFNSc41aar2"
      },
      "cell_type": "code",
      "source": [
        "#loading data from credits.csv for cast and crew, and Keywords.csv for keywords related to movies\n",
        "credits = pd.read_csv('credits.csv')\n",
        "keywords = pd.read_csv('keywords.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bafeMzEKaapU"
      },
      "cell_type": "code",
      "source": [
        "#Converting id's to int\n",
        "keywords['id'] = keywords['id'].astype('int')\n",
        "credits['id'] = credits['id'].astype('int')\n",
        "movies_metadata['id'] = movies_metadata['id'].astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Y42CJQoaamt"
      },
      "cell_type": "code",
      "source": [
        "# Add Cast and Crew column to our movies dataset\n",
        "movies_metadata = movies_metadata.merge(credits, on='id')\n",
        "#Add Keywords to the dataset\n",
        "movies_metadata = movies_metadata.merge(keywords, on='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcOluYrTaakJ"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ckzhupexaaho"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZb2iz0qbKRB"
      },
      "cell_type": "markdown",
      "source": [
        "After getting the data in a single dataframe, we can get the following from the data:\n",
        "<p><b> Crew:</b>\n",
        "Since director is the most important person in the crew of the movie, we will take it as our feature from the crew\n",
        "<p><b>  Cast: </b>\n",
        "We will take the first 3 actors from the Cast"
      ]
    },
    {
      "metadata": {
        "id": "lmZ301sLaae9"
      },
      "cell_type": "code",
      "source": [
        "#Checking for Python literal structures: strings, bytes, numbers, tuples, lists, dicts, sets, booleans, and None.\n",
        "movies_metadata['cast'] = movies_metadata['cast'].apply(literal_eval)\n",
        "movies_metadata['crew'] = movies_metadata['crew'].apply(literal_eval)\n",
        "movies_metadata['keywords'] = movies_metadata['keywords'].apply(literal_eval)\n",
        "#Get the cast and crew size\n",
        "movies_metadata['cast_size'] = movies_metadata['cast'].apply(lambda x: len(x))\n",
        "movies_metadata['crew_size'] = movies_metadata['crew'].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzx9H41baacX"
      },
      "cell_type": "code",
      "source": [
        "# function to get director from the dict of crew\n",
        "def get_director(d):\n",
        "    for i in d:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qChYM8EAaaZs"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['director'] = movies_metadata['crew'].apply(get_director)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-A2uVz55aaXG"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['cast'] = movies_metadata['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
        "movies_metadata['cast'] = movies_metadata['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oe_5zkuCaaUn"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['keywords'] = movies_metadata['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6NQ0t_6xbbu_"
      },
      "cell_type": "markdown",
      "source": [
        "We will add genre, keywords, director and main actors and create count matrix using count vectorizer as we did in Description based recommender and follow similar steps to calculate cosine similarities to get most similar movies.\n",
        "\n",
        "- Remove Spaces between names\n",
        "- Convert all features to lower case\n",
        "- This will help to distinguish between Christopher Nolen and Christopher Columbus\n",
        "- To get movies with same director more often, we will add director 3 times and provide additional weight to this feature"
      ]
    },
    {
      "metadata": {
        "id": "6V2M2Ua3aaRz"
      },
      "cell_type": "code",
      "source": [
        "#Remove spaces between names\n",
        "movies_metadata['cast'] = movies_metadata['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-M-mMMgaaPR"
      },
      "cell_type": "code",
      "source": [
        "#Remove spaces between names\n",
        "movies_metadata['director'] = movies_metadata['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\n",
        "#Add more weight to director\n",
        "movies_metadata['director'] = movies_metadata['director'].apply(lambda x: [x,x, x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQ0kPV-gaaMm"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yd-60MLxboM7"
      },
      "cell_type": "markdown",
      "source": [
        "<b>Keywords:</b>\n",
        "We only require keywords that occur more than once, having keywords that occur just once will increase complexity and reduce similarity score. So let's count the keywords and keep only those occuring more than once"
      ]
    },
    {
      "metadata": {
        "id": "FXxkkAf7aaJv"
      },
      "cell_type": "code",
      "source": [
        "k = movies_metadata.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "k.name = 'keyword'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CW12vUf3aaGe"
      },
      "cell_type": "code",
      "source": [
        "k = k.value_counts()\n",
        "k[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4cBNTOXaaDb"
      },
      "cell_type": "code",
      "source": [
        "#Removing keyword occuring just once\n",
        "k = k[k > 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwzpLeEwb0p5"
      },
      "cell_type": "markdown",
      "source": [
        "Using Snowball Stemmer, lets take the word back to its root form. This helps to reduce same features like forest and forests"
      ]
    },
    {
      "metadata": {
        "id": "o7fZYdd3aZ_5"
      },
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "stemmer.stem('forests')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXV2I0pbb35Z"
      },
      "cell_type": "code",
      "source": [
        "def filter_keywords(x):\n",
        "    words = []\n",
        "    for i in x:\n",
        "        if i in k:\n",
        "            words.append(i)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CeGZk9nIb32Y"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['keywords'] = movies_metadata['keywords'].apply(filter_keywords)\n",
        "movies_metadata['keywords'] = movies_metadata['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "movies_metadata['keywords'] = movies_metadata['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qcrMzAvb3zV"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata['analyzer'] = movies_metadata['keywords'] + movies_metadata['cast'] + movies_metadata['director'] + movies_metadata['genres']\n",
        "movies_metadata['analyzer'] = movies_metadata['analyzer'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhlw0R1sb3wX"
      },
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "count_matrix = count.fit_transform(movies_metadata['analyzer'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vUBa-eZhb3tb"
      },
      "cell_type": "code",
      "source": [
        "# Get pairwise cosine similarity\n",
        "cosine_sim = cosine_similarity(count_matrix, count_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S8gVQJx1b3ql"
      },
      "cell_type": "code",
      "source": [
        "movies_metadata = movies_metadata.reset_index()\n",
        "titles = movies_metadata['title']\n",
        "indexes = pd.Series(movies_metadata.index, index=movies_metadata['title'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RvMmeNgsb3n0"
      },
      "cell_type": "code",
      "source": [
        "desc_based_recommendation('Star Wars').head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CtAyXlgmcVIR"
      },
      "cell_type": "markdown",
      "source": [
        "We get much better results this time, most of the star war related movies are covered. Let's try for another movie 'Inception'"
      ]
    },
    {
      "metadata": {
        "id": "PwXA_EhZb3kw"
      },
      "cell_type": "code",
      "source": [
        "desc_based_recommendation('Inception').head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zLfHLOHcbsw"
      },
      "cell_type": "markdown",
      "source": [
        "This proves that adding weight to the director definetly works, as most of the movies in Top 10 is of Christopher Nolan"
      ]
    },
    {
      "metadata": {
        "id": "wlCdH8ySb3h5"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qykyeNUscmTT"
      },
      "cell_type": "markdown",
      "source": [
        "#### Popularity Based Recommendation\n",
        "Since our current recommender dosen't take popularity and ratings into account, it shows movies like 'Sky Captain and the World of Tomorrow' over many other popular movies.\n",
        "\n",
        "We will improve our recommendation system by returning only popular movies with more number of ratings\n",
        "\n",
        "let's take top 25 movies based on similarity scores and calculate the vote of the 70th percentile movie. Then, using this as the value of  m , we will calculate the weighted rating of each movie using IMDB's formula like we did in the Simple Recommender section."
      ]
    },
    {
      "metadata": {
        "id": "FkXy2FYbb3ex"
      },
      "cell_type": "code",
      "source": [
        "def popularity_based_recommendations(title,percentile=0.70):\n",
        "    idx = indexes[title]\n",
        "    sim = list(enumerate(cosine_sim[idx]))\n",
        "    sim = sorted(sim, key=lambda x: x[1], reverse=True)\n",
        "    sim = sim[1:26]\n",
        "    req_index = [i[0] for i in sim]\n",
        "\n",
        "    movies = movies_metadata.iloc[req_index][['title', 'vote_count', 'vote_average', 'year']]\n",
        "    no_of_votes = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "    vote_mean = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "    m = no_of_votes.quantile(percentile)\n",
        "    C = vote_mean.mean()\n",
        "    top_movies = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n",
        "    top_movies['vote_count'] = top_movies['vote_count'].astype('int')\n",
        "    top_movies['vote_average'] = top_movies['vote_average'].astype('int')\n",
        "    top_movies['wr'] = top_movies.apply(weighted_rating, axis=1)\n",
        "    top_movies = top_movies.sort_values('wr', ascending=False).head(25)\n",
        "    return top_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hVxaF_QWcsdF"
      },
      "cell_type": "code",
      "source": [
        "popularity_based_recommendations('Star Wars')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JoONjgXxcyeT"
      },
      "cell_type": "markdown",
      "source": [
        "We get even better recommendation using popularity based recommender, as we get X-Men and Iron Man 2 in the list , which are my favourites"
      ]
    },
    {
      "metadata": {
        "id": "0N-v9tpNcsZ1"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_si3wG4vc2vH"
      },
      "cell_type": "markdown",
      "source": [
        "#### Collaborative Filtering*\n",
        "The Results from our popularity based recommender are impressive, we get most of the similar movies when querying for a movie. While content based are good when we have good amount of content for the movie like the name of actors, movie synopsis, director’s information etc. we always don’t have all the information required for making relevant recommendations. Also, while we tried to derive user’s taste by using movies overview and taglines as input to our model, the recommendations provided by a collaborative filtering model are way better than a content based model. Another advantage of using a collaborative filtering model over Content based model is that it doesn’t require any data related to movies content. We have built a CF model using Scikit learn’s Surprise library which provides a simple data ingestion for making recommendations through CF. It also provides powerful algorithms like Singular Value Decomposition(SVD) to minimize RMSE and provide great recommendations."
      ]
    },
    {
      "metadata": {
        "id": "hdRtPNhtcsWS"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P7A_-JOXc-V0"
      },
      "cell_type": "markdown",
      "source": [
        "*The code for Collaborative filtering is referred from Rounak Banik's Github Repository which can be accessed [here](https://github.com/rounakbanik/movies/blob/master/movies_recommender.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "YbCBzDM0csS-"
      },
      "cell_type": "code",
      "source": [
        "reader = Reader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AIryM5cXcsPw"
      },
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('ratings.csv')\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1LfPu5wcsMv"
      },
      "cell_type": "code",
      "source": [
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "data.split(n_folds=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2QBAlYEcsJZ"
      },
      "cell_type": "code",
      "source": [
        "svd = SVD()\n",
        "evaluate(svd, data, measures=['RMSE', 'MAE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "szXUnMCQdPhu"
      },
      "cell_type": "markdown",
      "source": [
        "We get a mean Root Mean Sqaure Error of 0.8951 which is more than good enough for our case. Let us now train on our dataset and arrive at predictions."
      ]
    },
    {
      "metadata": {
        "id": "H1Ta9vMPcsGW"
      },
      "cell_type": "code",
      "source": [
        "trainset = data.build_full_trainset()\n",
        "svd.train(trainset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gqA8WwtScsCs"
      },
      "cell_type": "code",
      "source": [
        "#Provide userId, movieId and True Rating\n",
        "svd.predict(1, 302, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YvRm3jowdX-S"
      },
      "cell_type": "markdown",
      "source": [
        "For movie with ID 302, we get an estimated prediction in range of 2.5-3.0. One startling feature of this recommender system is that it doesn't care what the movie is (or what it contains). It works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have predicted the movie."
      ]
    },
    {
      "metadata": {
        "id": "7wTkgnJGcr_b"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDa1jlAAddkz"
      },
      "cell_type": "markdown",
      "source": [
        "#### Hybrid Recommender*\n",
        "\n",
        "Hybrid Recommender leverages the best of both Content based and collaborative filtering techniques.\n",
        "\n",
        "Input: User ID and the Title of a Movie\n",
        "Output: Similar movies sorted on the basis of expected ratings by that particular user.\n",
        "\n",
        "*Part of code for Hybrid Recommendation is referred from Rounak Banik's Github Repository which can be accessed from [here](https://github.com/rounakbanik/movies/blob/master/movies_recommender.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "ZlG6n3npcr8M"
      },
      "cell_type": "code",
      "source": [
        "links.drop('imdbId',axis=1,inplace=True)\n",
        "links.columns=['movieId', 'id']\n",
        "id_map = links.merge(movies_metadata[['title', 'id']], on='id').set_index('title'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J92-njgmcr5C"
      },
      "cell_type": "code",
      "source": [
        "indices_map = id_map.set_index('id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DJFGdLi3cr11"
      },
      "cell_type": "code",
      "source": [
        "def hybrid(userId, title):\n",
        "    idx = indexes[title]\n",
        "    tmdbId = id_map.loc[title]['id']\n",
        "    #print(idx)\n",
        "    movie_id = id_map.loc[title]['movieId']\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:26]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    movies = movies_metadata.iloc[movie_indices][['title', 'vote_count','year', 'id']]\n",
        "    movies['est rating'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n",
        "    movies = movies.sort_values('est rating', ascending=False)\n",
        "    return movies.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSBUg-Epcryu"
      },
      "cell_type": "code",
      "source": [
        "hybrid(1, 'Avatar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qK6FYN_Pcrvt"
      },
      "cell_type": "code",
      "source": [
        "hybrid(500, 'Avatar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9ErImU-dwLE"
      },
      "cell_type": "markdown",
      "source": [
        "We see that for our hybrid recommender, we get different recommendations for different users although the movie is the same. Hence, our recommendations are more personalized and tailored towards particular users"
      ]
    },
    {
      "metadata": {
        "id": "-GuVL12Tcrq2"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSDUUaNed0gr"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "<p>In this project, I have created 6 types of movie recommender systems:</p>\n",
        "<p><b>Simple Recommendaion System:</b></p>\n",
        "- We created Top Movies Charts based on Genre and utilized IMDB's Weighted Rating System to calculate ratings which was used to then sort and return top movies.\n",
        "\n",
        "<p><b>Content Based Recommendation System:</b> We built four content based recommendation engines</p>\n",
        "- First we gathered movie's overviews which a user has already seen and rated above average, then we used latent semantic similarity to get the similarity score and created a recommender that provides most similar story to user's liking.\n",
        "- On our second approach on creating taste based recommendation by using NLP techniques used for above, and added tagline to the description as an input\n",
        "- Next we considered metadata such as cast, crew, genre and keywords as input features to our Recommendation Engine, We also added weights features like director to get more similar results\n",
        "- We then improved our prediction by adding a popularity and ratings filter so that recommendations are given on popular movies\n",
        "\n",
        "<p><b>Collaborative Filtering Recommendation System:</b></p>\n",
        "- We used the powerful Surprise Library to build a collaborative filter based on single value decomposition(SVD). The RMSE obtained was less than 1 and the engine gave estimated ratings for a given user and movie.\n",
        "\n",
        "<p><b>Hybrid Recommendation System:</b></p>\n",
        "- Using ideas from Content based engine and Collaborative filtering based engine, we created a Hybrid recommender system which provided more personalized recommendations for users"
      ]
    },
    {
      "metadata": {
        "id": "sT38CfAQcrnS"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D2uk403Ud-fF"
      },
      "cell_type": "markdown",
      "source": [
        "The text in the document by Jai Soni is licensed under CC BY 3.0 https://creativecommons.org/licenses/by/3.0/us/\n",
        "\n",
        "The code in the document by Jai Soni is licensed under the MIT License https://opensource.org/licenses/MIT"
      ]
    },
    {
      "metadata": {
        "id": "o5M-WnkGcrjW"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohHagkkgeEAP"
      },
      "cell_type": "markdown",
      "source": [
        "## References:\n",
        "\n",
        "- [1] Kordik, Pavel,https://medium.com/recombee-blog/recommender-systems-explained-d98e8221f468\n",
        "- [2] Manoj Kumar, D.K. Yadav, Ankur Singh, Vijay Kr. Gupta. International Journal of Computer Applications (0975 – 8887) Volume  124 – No.3, August 2015\n",
        "- [3] https://en.wikipedia.org/wiki/Recommender_system\n",
        "- [4] Recommender System for News Articles using Supervised Learning - Scientific Figure on ResearchGate. Available from: https://www.researchgate.net/Collaborative-Vs-Content-Based-Filtering_fig3_318129942 [accessed 21 Apr, 2018]\n",
        "- [5] Banik, Rounak, https://www.datacamp.com/community/tutorials/recommender-systems-python\n",
        "- [6] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872 , Link for dataset https://grouplens.org/datasets/movielens/\n",
        "- [7] https://math.stackexchange.com/questions/169032/understanding-the-imdb-weighted-rating-function-for-usage-on-my-own-website\n",
        "- [8] http://language.worldofcomputing.net/category/tokenization\n",
        "- [9] https://pythonprogramming.net/stemming-nltk-tutorial/\n",
        "- [10] http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
        "- [11] https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "- [12] Susan T. Dumais (2005). \"Latent Semantic Analysis\". Annual Review of Information Science and Technology. 38: 188–230. doi:10.1002/aris.1440380105.\n",
        "- [13] Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield and Johnathan Weese, UMBC_EBIQUITY-CORE: Semantic Textual Similarity Systems, Proc. 2nd Joint Conf. on Lexical and Computational Semantics, Association for Computational Linguistics, June 2013.\n",
        "- [14]Christian S. Perone http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/\n",
        "- [15] Banik, Rounak https://github.com/rounakbanik/movies/blob/master/movies_recommender.ipynb\n",
        "- [16] Image Source: http://bgr.com/2016/03/18/find-movie-forgot-name-using-description/"
      ]
    },
    {
      "metadata": {
        "id": "N0mDORItb3be"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z72x-6dveInb"
      },
      "cell_type": "markdown",
      "source": [
        "### How to use this dataset while running the google colab\n",
        "- upload the datset in files parallel to sample_data(all ready presented in colab)"
      ]
    }
  ]
}